# ğŸ§  TIPE â€“ Neural networks are universal approximators

Ce dÃ©pÃ´t prÃ©sente les expÃ©riences numÃ©riques et visualisations rÃ©alisÃ©es dans le cadre de mon **TIPE (Travail d'Initiative Personnelle EncadrÃ©)** en classe prÃ©paratoire MPSI.

---

## ğŸ¯ Sujet

**Les rÃ©seaux de neurones comme approximateurs universels**  
Ce projet explore le thÃ©orÃ¨me dâ€™approximation universelle de Cybenko (1989), qui affirme quâ€™un **rÃ©seau de neurones Ã  une seule couche cachÃ©e**, avec fonction dâ€™activation sigmoÃ¯de, peut approximer **nâ€™importe quelle fonction continue sur un compact**, avec une prÃ©cision arbitraire.

---

## ğŸ”¬ Objectifs du projet

- Comprendre et illustrer le thÃ©orÃ¨me mathÃ©matique d'approximation universelle
- ImplÃ©menter des rÃ©seaux Ã  1 couche cachÃ©e avec PyTorch
- Visualiser lâ€™effet du **nombre de neurones** et de la **forme de lâ€™activation**
- Approcher diffÃ©rentes fonctions (ex: sin(x), |x|, indicatrice, xÂ²)
- InterprÃ©ter graphiquement la qualitÃ© dâ€™approximation

---

## ğŸ› ï¸ Technologies

- [Python 3.10+](https://www.python.org)
- [PyTorch](https://pytorch.org)
- [matplotlib](https://matplotlib.org)
- NumPy

---

## ğŸ§  Ã€ retenir

> Un simple rÃ©seau Ã  une couche, avec assez de neurones et une activation non-linÃ©aire appropriÃ©e, peut approximer **nâ€™importe quelle fonction continue**.  
Ce projet en donne une **dÃ©monstration numÃ©rique accessible** et visuelle.

---

## ğŸ‘¨â€ğŸ’» Auteur

InspirÃ© par le travail de ThÃ©odore Hoynck.

Charles Ottaviani
Ã‰tudiant en MPSI/MP* â€“ TIPE 2025  
[LinkedIn](https://www.linkedin.com/in/charles-ottaviani)

---

## ğŸ“ Ã€ venir

- Approximations discontinues
- Etude de la loss

